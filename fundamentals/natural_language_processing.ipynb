{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TonyQ2k3/pytorch-deeplearning/blob/main/fundamentals/natural_language_processing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kyfCIe1JLiNo"
      },
      "source": [
        "# Natural Language Processing for PyTorch"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tasks of NLP\n",
        "1. Text Classification: Classify text fragments into classes to detect types of messages, along with the intent of the user\n",
        "2. Sentiment Analysis: Determine the degree of negativity/positivity of a piece of text.\n",
        "3. Named Entity Recognition: Recognizing and filtering name, address etc from a piece of text\n",
        "4. Summarization: Find the most meaningful information inside texts\n",
        "5. QnA: Extract info from data, along with extracting piece of questions to be used as tags"
      ],
      "metadata": {
        "id": "VLOF8fLCL7lk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Representing data\n",
        "Like any other tensor, text needs to be represented as tensors. Here are some approaches for that.\n",
        "1. Character-level representation: Treat each character as a number, e.g the ID of the letter inside a text corpus. If we have ùêç characters in our text corpus, a string of ùò§ characters will be represented by a tensor of size ùò§ ùòÖ ùêç.\n",
        "2. Word-level representation: Create a vocabulary of all words in our string. This simplify the task for the model for words have more meaning than letters. But dealing with huge number of words is stressful.\n",
        "\n",
        "‚áí Unify 2 approaches by calling an atomic piece of text a *token*. A token may be a letter but also a word. Converting text into sequences of tokens is called *tokenization*. Then we assign each token a number and feed into the neural network, this is called *vectorization*."
      ],
      "metadata": {
        "id": "dtohjCC6L_6Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Text classification"
      ],
      "metadata": {
        "id": "GcANHO7ca1wr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import os\n",
        "import collections"
      ],
      "metadata": {
        "id": "nC_XSjD0bC_l"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fBl_-iQzbL68"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}